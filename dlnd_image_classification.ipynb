{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 2:\n",
      "Image - Min Value: 20 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 9 Name: truck\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCXoAKEzQA0Bhgh4AChP0AFDY/wb1gCgl/MrBSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9959be35c0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 2\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    normalized_x_min = 0.0\n",
    "    normalized_x_max = 1.0\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    \n",
    "    normalized_x = normalized_x_min + ( \n",
    "        ( (x - grayscale_min)*\n",
    "        (normalized_x_max - normalized_x_min) )/\n",
    "        ( grayscale_max - grayscale_min ))       \n",
    "    \n",
    "    return normalized_x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"    \n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"    \n",
    "    x = tf.placeholder(tf.float32,shape=[None,*image_shape], name=\"x\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"    \n",
    "    y = tf.placeholder(tf.float32,shape=[None,n_classes], name=\"y\")\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"    \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function   \n",
    "    _, input_height, input_width, input_depth = x_tensor.get_shape().as_list()\n",
    "    W = tf.Variable(tf.truncated_normal(shape=[*conv_ksize, input_depth, conv_num_outputs],stddev = 0.1))\n",
    "    B = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    conv = tf.nn.conv2d(x_tensor, W, [1,*conv_strides,1], padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv,B)\n",
    "    relu = tf.nn.relu(conv)    \n",
    "    max_pooling = tf.nn.max_pool(relu,[1, *pool_ksize, 1],[1,*pool_strides,1], padding='SAME')\n",
    "    return max_pooling \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"    \n",
    "    fully_connected = tf.contrib.layers.fully_connected(x_tensor,num_outputs, activation_fn=tf.nn.relu)\n",
    "    return fully_connected\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    output_W = tf.Variable(tf.truncated_normal(shape=[x_tensor.get_shape().as_list()[1],num_outputs]))\n",
    "    output_B = tf.Variable(tf.zeros(num_outputs))  \n",
    "    output = tf.add(tf.matmul(x_tensor, output_W), output_B)\n",
    "    #output = tf.layers.dense(inputs=x_tensor, units=num_outputs, activation=None)\n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:    \n",
    "    #print(x)\n",
    "    #Tensor(\"Placeholder:0\", shape=(?, 32, 32, 5), dtype=float32)\n",
    "    #10\n",
    "    #(2, 2)\n",
    "    #(4, 4)\n",
    "    #(2, 2)\n",
    "    #(2, 2)\n",
    "    \n",
    "    #conv_num_outputs = [8,64,128]\n",
    "    #conv_ksize = ((2,2), (3,3), (8,8))\n",
    "    #conv_strides = ((1,1), (1,1), (1,1))\n",
    "    #pool_ksize = ((2,2), (3,3), (8,8))\n",
    "    #pool_strides = ((1,1), (1,1), (1,1))\n",
    "    #num_outputs = [324, 64,10]\n",
    "    \n",
    "    conv_num_outputs = 100\n",
    "    conv_ksize = [(4, 4),(6, 6), (8, 8),(10, 10) ]\n",
    "    conv_strides = [(3, 3),(4, 4),(6, 6),(10, 10)]\n",
    "    pool_ksize = [(4, 4),(8, 8),(10, 10),(10, 10)]\n",
    "    pool_strides = [(4, 4),(6, 6),(8, 8),(10, 10)]\n",
    "    \n",
    "    t1 = conv2d_maxpool(x, conv_num_outputs, conv_ksize[0], conv_strides[0], pool_ksize[0], pool_strides[0])    \n",
    "    t2 = conv2d_maxpool(t1, conv_num_outputs, conv_ksize[1], conv_strides[1], pool_ksize[1], pool_strides[1])\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    flatten_tensor = flatten(t2)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    num_outputs = 10\n",
    "    \n",
    "    f1 = fully_conn(flatten_tensor, 1000)    \n",
    "    f2 = fully_conn(f1, 1000)    \n",
    "    d1 = tf.nn.dropout(f2,keep_prob)\n",
    "    f3 = fully_conn(d1, 1000)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    output_tensor = output(f3, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return output_tensor\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    result = session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(sess, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"    \n",
    "    #acc = session.run(accuracy) \n",
    "    #print(\"Accuracy: \" + str(acc))\n",
    " \n",
    "    loss = sess.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={x: valid_features, y: valid_labels,  keep_prob: 1})\n",
    "    print('loss', loss, 'accuracy', valid_acc)\n",
    "\n",
    "    #print(\"Features: \" + str(feature_batch))\n",
    "    #print(\"\")\n",
    "    #print(\"Labels: \" + str(label_batch))\n",
    "    #print(type(accuracy))\n",
    "    #print(type(cost))\n",
    "    #print(\"Cost: \" + str(cost))\n",
    "    #print(\"Accuracy: \" + str(total))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 60\n",
    "batch_size = 256\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss 2.22544 accuracy 0.1262\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss 2.14795 accuracy 0.1792\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss 2.13089 accuracy 0.2216\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss 2.11345 accuracy 0.249\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss 2.08619 accuracy 0.2682\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss 2.05172 accuracy 0.2832\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss 1.99782 accuracy 0.2972\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss 1.93033 accuracy 0.3022\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss 1.87763 accuracy 0.3172\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss 1.88277 accuracy 0.3222\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss 1.83635 accuracy 0.3332\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss 1.80918 accuracy 0.3544\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss 1.82246 accuracy 0.3582\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss 1.77184 accuracy 0.3786\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss 1.7039 accuracy 0.3962\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss 1.6673 accuracy 0.376\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss 1.62544 accuracy 0.3822\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss 1.55395 accuracy 0.3952\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss 1.59113 accuracy 0.3838\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss 1.55157 accuracy 0.381\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss 1.51571 accuracy 0.4064\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss 1.5393 accuracy 0.3982\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss 1.4357 accuracy 0.4128\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss 1.39623 accuracy 0.4156\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss 1.36926 accuracy 0.415\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss 1.31865 accuracy 0.4162\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss 1.31141 accuracy 0.4144\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss 1.27133 accuracy 0.4178\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss 1.24853 accuracy 0.417\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss 1.21415 accuracy 0.4118\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss 1.20433 accuracy 0.4074\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss 1.22109 accuracy 0.3918\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss 1.21482 accuracy 0.412\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss 1.17961 accuracy 0.4224\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss 1.17357 accuracy 0.4162\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss 1.13986 accuracy 0.4274\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss 1.12035 accuracy 0.4214\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss 1.08838 accuracy 0.4292\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss 1.0979 accuracy 0.4222\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss 1.0725 accuracy 0.4262\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss 1.05093 accuracy 0.4356\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss 1.02951 accuracy 0.4364\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss 1.04897 accuracy 0.421\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss 1.07093 accuracy 0.411\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss 1.05023 accuracy 0.4058\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss 0.967954 accuracy 0.4244\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss 1.01796 accuracy 0.4112\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss 0.985481 accuracy 0.4072\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss 1.02213 accuracy 0.4204\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss 0.928492 accuracy 0.4274\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss 0.904882 accuracy 0.4402\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss 0.925018 accuracy 0.4438\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss 0.887956 accuracy 0.4476\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss 0.899264 accuracy 0.4426\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss 0.856302 accuracy 0.4402\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss 0.836744 accuracy 0.444\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss 0.820009 accuracy 0.4482\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss 0.788461 accuracy 0.4392\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss 0.780963 accuracy 0.4508\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss 0.738764 accuracy 0.4488\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss 0.716511 accuracy 0.4536\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss 0.71912 accuracy 0.4502\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss 0.725272 accuracy 0.4462\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss 0.73647 accuracy 0.443\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss 0.709521 accuracy 0.4482\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss 0.732445 accuracy 0.4416\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss 0.722415 accuracy 0.4396\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss 0.70318 accuracy 0.4388\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss 0.647959 accuracy 0.4464\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss 0.655344 accuracy 0.443\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss 0.673193 accuracy 0.4428\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss 0.739931 accuracy 0.4298\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss 0.67541 accuracy 0.4334\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss 0.652945 accuracy 0.4416\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss 0.672322 accuracy 0.4404\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss 0.639731 accuracy 0.4458\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss 0.641853 accuracy 0.4432\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss 0.571363 accuracy 0.4392\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss 0.596183 accuracy 0.4334\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss 0.556744 accuracy 0.4364\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss 0.569174 accuracy 0.4238\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss 0.5087 accuracy 0.4396\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss 0.497286 accuracy 0.4416\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss 0.475463 accuracy 0.4368\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss 0.473542 accuracy 0.4406\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss 0.506661 accuracy 0.4326\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss 0.473118 accuracy 0.4364\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss 0.509459 accuracy 0.439\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss 0.475375 accuracy 0.4374\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss 0.467961 accuracy 0.444\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss 0.435566 accuracy 0.4394\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss 0.451188 accuracy 0.435\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss 0.473103 accuracy 0.4416\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss 0.431832 accuracy 0.4416\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss 0.472144 accuracy 0.4362\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss 0.498382 accuracy 0.4182\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss 0.40949 accuracy 0.4304\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss 0.351911 accuracy 0.4418\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss 0.401568 accuracy 0.4244\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss 0.372907 accuracy 0.4244\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss 2.13766 accuracy 0.1936\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss 2.10756 accuracy 0.2262\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss 1.86961 accuracy 0.2046\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss 1.92049 accuracy 0.2204\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss 1.91682 accuracy 0.2636\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss 2.1123 accuracy 0.2654\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss 1.94175 accuracy 0.3036\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss 1.77097 accuracy 0.2718\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss 1.82863 accuracy 0.3106\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss 1.79962 accuracy 0.3244\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss 2.07744 accuracy 0.333\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss 1.89692 accuracy 0.3284\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss 1.64263 accuracy 0.3278\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss 1.6784 accuracy 0.3514\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss 1.65681 accuracy 0.3606\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss 1.95136 accuracy 0.362\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss 1.73762 accuracy 0.367\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss 1.54588 accuracy 0.3638\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss 1.57613 accuracy 0.3966\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss 1.55067 accuracy 0.384\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss 1.9351 accuracy 0.394\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss 1.60639 accuracy 0.3966\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss 1.45162 accuracy 0.3836\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss 1.5679 accuracy 0.4066\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss 1.50843 accuracy 0.4038\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss 1.88341 accuracy 0.4098\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss 1.51769 accuracy 0.4098\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss 1.34921 accuracy 0.4218\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss 1.49136 accuracy 0.439\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss 1.55436 accuracy 0.4254\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss 1.76434 accuracy 0.4232\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss 1.40258 accuracy 0.429\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss 1.28962 accuracy 0.4298\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss 1.42649 accuracy 0.435\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss 1.4732 accuracy 0.452\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss 1.69983 accuracy 0.4348\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss 1.35254 accuracy 0.4132\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss 1.22788 accuracy 0.4326\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss 1.34936 accuracy 0.4524\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss 1.40528 accuracy 0.4552\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss 1.58866 accuracy 0.459\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss 1.29164 accuracy 0.4388\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss 1.16278 accuracy 0.4514\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss 1.2748 accuracy 0.4498\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss 1.37709 accuracy 0.4682\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss 1.54543 accuracy 0.4526\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss 1.26151 accuracy 0.4384\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss 1.10652 accuracy 0.4616\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss 1.24999 accuracy 0.4604\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss 1.36664 accuracy 0.4682\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss 1.46232 accuracy 0.4648\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss 1.23454 accuracy 0.446\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss 1.03371 accuracy 0.4682\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss 1.21714 accuracy 0.4722\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss 1.31088 accuracy 0.4728\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss 1.38006 accuracy 0.4608\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss 1.13603 accuracy 0.464\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss 1.00371 accuracy 0.4806\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss 1.17271 accuracy 0.4662\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss 1.29725 accuracy 0.4796\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss 1.31403 accuracy 0.4696\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss 1.11939 accuracy 0.469\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss 0.946058 accuracy 0.4844\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss 1.16987 accuracy 0.4626\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss 1.24026 accuracy 0.4802\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss 1.29743 accuracy 0.4784\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss 1.07765 accuracy 0.4758\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss 0.942818 accuracy 0.4794\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss 1.10053 accuracy 0.4708\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss 1.20731 accuracy 0.4866\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss 1.29427 accuracy 0.4754\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss 1.02684 accuracy 0.4602\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss 0.946635 accuracy 0.4698\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss 1.0591 accuracy 0.482\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss 1.18325 accuracy 0.4862\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss 1.20637 accuracy 0.4822\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss 0.995401 accuracy 0.4626\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss 0.914048 accuracy 0.474\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss 1.024 accuracy 0.489\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss 1.17429 accuracy 0.4806\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss 1.19269 accuracy 0.48\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss 0.937993 accuracy 0.4682\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss 0.90966 accuracy 0.4818\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss 0.976759 accuracy 0.4844\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss 1.12614 accuracy 0.4876\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss 1.14655 accuracy 0.4858\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss 0.942432 accuracy 0.4594\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss 0.865969 accuracy 0.4838\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss 0.970608 accuracy 0.4934\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss 1.10001 accuracy 0.4988\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss 1.1312 accuracy 0.4972\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss 0.888846 accuracy 0.4828\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss 0.83453 accuracy 0.49\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss 0.946006 accuracy 0.4898\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss 1.02798 accuracy 0.5004\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss 1.06992 accuracy 0.4958\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss 0.864075 accuracy 0.4742\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss 0.812532 accuracy 0.4904\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss 0.90278 accuracy 0.4994\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss 0.963694 accuracy 0.5044\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss 1.04262 accuracy 0.4976\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss 0.846838 accuracy 0.4706\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss 0.779708 accuracy 0.5006\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss 0.86454 accuracy 0.4992\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss 0.920624 accuracy 0.5046\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss 0.999277 accuracy 0.4982\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss 0.823048 accuracy 0.4736\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss 0.755613 accuracy 0.5142\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss 0.846105 accuracy 0.5036\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss 0.888672 accuracy 0.5014\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss 0.962556 accuracy 0.5066\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss 0.790336 accuracy 0.4756\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss 0.730569 accuracy 0.5088\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss 0.809373 accuracy 0.4992\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss 0.867598 accuracy 0.5082\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss 0.939477 accuracy 0.5022\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss 0.781173 accuracy 0.4698\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss 0.730163 accuracy 0.5092\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss 0.79708 accuracy 0.505\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss 0.838082 accuracy 0.5064\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss 0.923806 accuracy 0.5012\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss 0.752673 accuracy 0.4776\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss 0.737457 accuracy 0.4942\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss 0.75956 accuracy 0.5032\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss 0.841689 accuracy 0.5112\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss 0.905628 accuracy 0.5072\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss 0.789319 accuracy 0.4842\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss 0.683762 accuracy 0.5072\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss 0.759089 accuracy 0.503\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss 0.826003 accuracy 0.4996\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss 0.894489 accuracy 0.5012\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss 0.811452 accuracy 0.4958\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss 0.666198 accuracy 0.5136\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss 0.756331 accuracy 0.4912\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss 0.777643 accuracy 0.507\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss 0.895724 accuracy 0.4926\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss 0.798106 accuracy 0.5228\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss 0.671132 accuracy 0.5084\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss 0.731836 accuracy 0.4998\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss 0.740008 accuracy 0.513\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss 0.80816 accuracy 0.5064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, CIFAR-10 Batch 2:  loss 0.739199 accuracy 0.518\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss 0.656318 accuracy 0.5004\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss 0.698071 accuracy 0.501\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss 0.670766 accuracy 0.5182\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss 0.815227 accuracy 0.5098\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss 0.730141 accuracy 0.5172\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss 0.623507 accuracy 0.5052\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss 0.696763 accuracy 0.4944\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss 0.768707 accuracy 0.502\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss 0.857289 accuracy 0.4974\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss 0.715943 accuracy 0.5066\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss 0.628354 accuracy 0.5056\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss 0.682393 accuracy 0.4846\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss 0.73608 accuracy 0.5036\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss 0.79113 accuracy 0.5034\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss 0.643219 accuracy 0.5014\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss 0.578048 accuracy 0.5052\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss 0.58657 accuracy 0.5058\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss 0.690141 accuracy 0.5074\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss 0.75035 accuracy 0.505\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss 0.610291 accuracy 0.5014\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss 0.54175 accuracy 0.511\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss 0.554368 accuracy 0.5122\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss 0.631764 accuracy 0.515\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss 0.733078 accuracy 0.5102\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss 0.598335 accuracy 0.5064\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss 0.521519 accuracy 0.5056\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss 0.53649 accuracy 0.517\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss 0.594876 accuracy 0.518\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss 0.659939 accuracy 0.5134\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss 0.578322 accuracy 0.5078\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss 0.510382 accuracy 0.5126\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss 0.493065 accuracy 0.5158\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss 0.53645 accuracy 0.52\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss 0.668743 accuracy 0.5082\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss 0.582038 accuracy 0.508\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss 0.494589 accuracy 0.5208\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss 0.441945 accuracy 0.52\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss 0.501575 accuracy 0.5148\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss 0.603045 accuracy 0.5074\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss 0.545486 accuracy 0.5106\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss 0.470825 accuracy 0.508\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss 0.43654 accuracy 0.5184\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss 0.482755 accuracy 0.511\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss 0.571769 accuracy 0.5026\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss 0.512176 accuracy 0.5012\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss 0.491487 accuracy 0.5046\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss 0.431816 accuracy 0.521\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss 0.451753 accuracy 0.5088\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss 0.57101 accuracy 0.5026\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss 0.486504 accuracy 0.4996\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss 0.467002 accuracy 0.503\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss 0.414121 accuracy 0.526\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss 0.504782 accuracy 0.4982\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss 0.520173 accuracy 0.5218\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss 0.526952 accuracy 0.5146\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss 0.477188 accuracy 0.5038\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss 0.403861 accuracy 0.522\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss 0.433774 accuracy 0.5058\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss 0.565328 accuracy 0.5164\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss 0.515331 accuracy 0.5154\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss 0.468804 accuracy 0.4956\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss 0.435277 accuracy 0.5114\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss 0.419742 accuracy 0.51\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss 0.530846 accuracy 0.502\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss 0.459324 accuracy 0.5088\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss 0.446385 accuracy 0.4926\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss 0.399038 accuracy 0.5066\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss 0.437137 accuracy 0.5098\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss 0.472678 accuracy 0.5068\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss 0.452773 accuracy 0.515\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss 0.40964 accuracy 0.5022\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss 0.36898 accuracy 0.5082\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss 0.431202 accuracy 0.5088\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss 0.438673 accuracy 0.5092\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss 0.441574 accuracy 0.5072\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss 0.378289 accuracy 0.496\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss 0.346232 accuracy 0.51\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss 0.411083 accuracy 0.5068\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss 0.412807 accuracy 0.5142\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss 0.435702 accuracy 0.509\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss 0.373754 accuracy 0.5026\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss 0.366005 accuracy 0.5222\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss 0.382658 accuracy 0.508\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss 0.409242 accuracy 0.4996\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss 0.409375 accuracy 0.5114\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss 0.349908 accuracy 0.5\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss 0.326956 accuracy 0.5182\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss 0.348471 accuracy 0.5042\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss 0.362015 accuracy 0.5072\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss 0.352062 accuracy 0.5184\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss 0.318457 accuracy 0.5054\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss 0.295816 accuracy 0.5152\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss 0.308095 accuracy 0.5058\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss 0.306866 accuracy 0.4998\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss 0.357774 accuracy 0.5186\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss 0.320424 accuracy 0.4956\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss 0.323102 accuracy 0.5132\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss 0.277145 accuracy 0.5028\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss 0.30431 accuracy 0.5084\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss 0.357765 accuracy 0.5116\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss 0.322502 accuracy 0.4852\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss 0.285643 accuracy 0.5096\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss 0.276433 accuracy 0.5162\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss 0.277847 accuracy 0.5092\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss 0.365606 accuracy 0.516\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss 0.295269 accuracy 0.4966\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss 0.291601 accuracy 0.5072\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss 0.291819 accuracy 0.517\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss 0.277564 accuracy 0.503\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss 0.349627 accuracy 0.516\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss 0.261227 accuracy 0.5048\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss 0.247507 accuracy 0.5112\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss 0.275615 accuracy 0.515\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss 0.236883 accuracy 0.5044\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss 0.319199 accuracy 0.5182\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss 0.244819 accuracy 0.5064\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss 0.232276 accuracy 0.5114\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss 0.264287 accuracy 0.5226\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss 0.239587 accuracy 0.5038\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss 0.333474 accuracy 0.5124\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss 0.24614 accuracy 0.5188\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss 0.193097 accuracy 0.5218\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss 0.261852 accuracy 0.5276\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss 0.23729 accuracy 0.4958\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss 0.271532 accuracy 0.517\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss 0.223498 accuracy 0.5138\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss 0.181974 accuracy 0.5244\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss 0.260318 accuracy 0.5164\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss 0.202436 accuracy 0.5128\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss 0.321418 accuracy 0.5156\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss 0.238682 accuracy 0.5136\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss 0.15747 accuracy 0.5218\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss 0.235169 accuracy 0.5236\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss 0.178286 accuracy 0.5066\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss 0.285354 accuracy 0.4988\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss 0.230453 accuracy 0.5128\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss 0.182535 accuracy 0.5176\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss 0.24997 accuracy 0.5158\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss 0.228373 accuracy 0.5114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, CIFAR-10 Batch 2:  loss 0.329069 accuracy 0.478\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss 0.238877 accuracy 0.5134\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss 0.151454 accuracy 0.5168\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss 0.269459 accuracy 0.5064\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss 0.232254 accuracy 0.5164\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss 0.31925 accuracy 0.48\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss 0.233115 accuracy 0.5188\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss 0.172979 accuracy 0.5182\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss 0.248473 accuracy 0.505\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss 0.258409 accuracy 0.5164\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss 0.291807 accuracy 0.4824\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss 0.19191 accuracy 0.5246\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss 0.179884 accuracy 0.5156\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss 0.197401 accuracy 0.5072\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss 0.246006 accuracy 0.501\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss 0.302502 accuracy 0.4922\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss 0.235067 accuracy 0.5166\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss 0.163139 accuracy 0.5098\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss 0.211859 accuracy 0.5058\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss 0.201245 accuracy 0.5052\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss 0.266642 accuracy 0.4782\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss 0.163837 accuracy 0.52\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss 0.15856 accuracy 0.5066\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss 0.190504 accuracy 0.5102\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss 0.213996 accuracy 0.4972\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss 0.284241 accuracy 0.4888\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss 0.199145 accuracy 0.508\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss 0.178476 accuracy 0.5\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss 0.212141 accuracy 0.5112\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss 0.239965 accuracy 0.4922\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss 0.252691 accuracy 0.4892\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss 0.18154 accuracy 0.5046\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss 0.16889 accuracy 0.498\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss 0.219934 accuracy 0.5148\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss 0.170077 accuracy 0.4966\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss 0.219941 accuracy 0.4812\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss 0.144372 accuracy 0.5014\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss 0.211428 accuracy 0.4848\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss 0.212317 accuracy 0.5144\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss 0.160163 accuracy 0.4956\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss 0.234242 accuracy 0.4748\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss 0.156597 accuracy 0.5046\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss 0.183579 accuracy 0.492\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss 0.207124 accuracy 0.5082\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss 0.151342 accuracy 0.4994\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss 0.218221 accuracy 0.4956\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss 0.125776 accuracy 0.5036\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss 0.162518 accuracy 0.4978\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss 0.172225 accuracy 0.5026\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss 0.140675 accuracy 0.5086\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss 0.22718 accuracy 0.4914\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss 0.164567 accuracy 0.4916\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss 0.143826 accuracy 0.5042\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss 0.150813 accuracy 0.5094\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss 0.154739 accuracy 0.4964\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss 0.284201 accuracy 0.493\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss 0.129731 accuracy 0.4844\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss 0.104861 accuracy 0.5058\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss 0.124265 accuracy 0.5172\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss 0.125081 accuracy 0.4934\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss 0.22552 accuracy 0.5002\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss 0.117888 accuracy 0.4876\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss 0.116437 accuracy 0.495\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss 0.134696 accuracy 0.515\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss 0.100678 accuracy 0.4998\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss 0.218746 accuracy 0.4958\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss 0.128039 accuracy 0.4866\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss 0.122576 accuracy 0.4916\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss 0.150038 accuracy 0.505\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss 0.109803 accuracy 0.4986\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss 0.190539 accuracy 0.499\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss 0.116981 accuracy 0.4852\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss 0.0964158 accuracy 0.4836\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss 0.133673 accuracy 0.5058\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss 0.112372 accuracy 0.4892\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss 0.217038 accuracy 0.494\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss 0.12097 accuracy 0.493\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss 0.123255 accuracy 0.4734\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss 0.121585 accuracy 0.4982\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss 0.130379 accuracy 0.4782\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss 0.222562 accuracy 0.5012\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss 0.0963943 accuracy 0.508\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss 0.10176 accuracy 0.4696\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss 0.122429 accuracy 0.5072\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss 0.102337 accuracy 0.4828\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss 0.240244 accuracy 0.4946\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss 0.102503 accuracy 0.4978\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss 0.113466 accuracy 0.4688\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss 0.137641 accuracy 0.4946\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss 0.161321 accuracy 0.4746\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss 0.223149 accuracy 0.4938\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss 0.112277 accuracy 0.5032\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss 0.145132 accuracy 0.4636\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss 0.155439 accuracy 0.493\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss 0.138732 accuracy 0.4812\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss 0.200435 accuracy 0.4904\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss 0.102293 accuracy 0.5064\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss 0.0949638 accuracy 0.4774\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss 0.136336 accuracy 0.4964\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss 0.133363 accuracy 0.4906\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss 0.194367 accuracy 0.4864\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss 0.0936788 accuracy 0.51\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss 0.125874 accuracy 0.4818\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss 0.118097 accuracy 0.4826\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss 0.159311 accuracy 0.4926\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss 0.217532 accuracy 0.4926\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss 0.112639 accuracy 0.5132\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss 0.106245 accuracy 0.4816\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss 0.15442 accuracy 0.488\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss 0.137808 accuracy 0.4904\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss 0.183243 accuracy 0.4848\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss 0.118789 accuracy 0.5116\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss 0.0889546 accuracy 0.4934\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss 0.136358 accuracy 0.5066\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss 0.108591 accuracy 0.4944\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss 0.194488 accuracy 0.4782\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss 0.13871 accuracy 0.5072\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss 0.0671809 accuracy 0.4966\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss 0.0881812 accuracy 0.5084\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss 0.103666 accuracy 0.5004\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss 0.153707 accuracy 0.4854\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss 0.136274 accuracy 0.4984\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss 0.0775028 accuracy 0.4986\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss 0.0730578 accuracy 0.5\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss 0.0966358 accuracy 0.4838\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss 0.127165 accuracy 0.4894\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss 0.12092 accuracy 0.5042\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss 0.0821868 accuracy 0.5076\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss 0.0572152 accuracy 0.509\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss 0.0690053 accuracy 0.5\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss 0.105501 accuracy 0.4836\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss 0.118962 accuracy 0.4942\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss 0.0698082 accuracy 0.5036\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss 0.0573672 accuracy 0.5124\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss 0.0795118 accuracy 0.495\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss 0.0996699 accuracy 0.4898\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss 0.0974085 accuracy 0.4906\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss 0.0669552 accuracy 0.5016\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss 0.0553545 accuracy 0.5072\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss 0.0681352 accuracy 0.4896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, CIFAR-10 Batch 2:  loss 0.0925145 accuracy 0.4816\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss 0.0871621 accuracy 0.4942\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss 0.065072 accuracy 0.4952\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss 0.0728503 accuracy 0.5112\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss 0.0594345 accuracy 0.489\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss 0.0905795 accuracy 0.4736\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss 0.075574 accuracy 0.4976\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss 0.0530138 accuracy 0.4962\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss 0.0545994 accuracy 0.5054\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss 0.0478619 accuracy 0.4996\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss 0.0896105 accuracy 0.4772\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss 0.0593348 accuracy 0.4894\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss 0.0634446 accuracy 0.4994\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss 0.0508752 accuracy 0.5016\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss 0.0433828 accuracy 0.496\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss 0.0836856 accuracy 0.4696\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss 0.0770839 accuracy 0.4916\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss 0.0827007 accuracy 0.488\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss 0.0535072 accuracy 0.5036\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss 0.0387081 accuracy 0.5048\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss 0.0869631 accuracy 0.485\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss 0.0588979 accuracy 0.4922\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss 0.0803411 accuracy 0.4852\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss 0.0716803 accuracy 0.4932\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss 0.0490223 accuracy 0.4986\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss 0.144582 accuracy 0.4904\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss 0.0573585 accuracy 0.4912\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss 0.0686683 accuracy 0.4902\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss 0.0653019 accuracy 0.4942\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss 0.033974 accuracy 0.4974\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss 0.0803623 accuracy 0.4948\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss 0.0779483 accuracy 0.4876\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss 0.0538707 accuracy 0.4968\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss 0.0755607 accuracy 0.495\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss 0.0447267 accuracy 0.4986\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss 0.1085 accuracy 0.4832\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss 0.0626623 accuracy 0.4796\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss 0.0562982 accuracy 0.4924\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss 0.0527284 accuracy 0.4916\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss 0.0345144 accuracy 0.4962\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss 0.121619 accuracy 0.4952\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss 0.0781685 accuracy 0.4824\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss 0.0674829 accuracy 0.494\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss 0.0478863 accuracy 0.4928\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss 0.0501238 accuracy 0.483\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss 0.129957 accuracy 0.495\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss 0.0832526 accuracy 0.4854\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss 0.0937239 accuracy 0.491\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss 0.0302239 accuracy 0.4914\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss 0.0339638 accuracy 0.4966\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss 0.0826534 accuracy 0.4954\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss 0.0818729 accuracy 0.4948\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss 0.0760649 accuracy 0.4832\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss 0.0452499 accuracy 0.488\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss 0.0373369 accuracy 0.4966\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss 0.083521 accuracy 0.4848\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss 0.0819147 accuracy 0.4942\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss 0.0878819 accuracy 0.4786\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss 0.0415174 accuracy 0.483\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss 0.0483331 accuracy 0.487\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss 0.0689789 accuracy 0.4856\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss 0.0440707 accuracy 0.492\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss 0.0737701 accuracy 0.4846\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss 0.0432317 accuracy 0.483\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss 0.0306649 accuracy 0.4848\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss 0.0717856 accuracy 0.483\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss 0.0545533 accuracy 0.4882\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss 0.0805737 accuracy 0.4788\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss 0.0604126 accuracy 0.4626\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss 0.0380833 accuracy 0.486\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss 0.0707991 accuracy 0.4682\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss 0.0398781 accuracy 0.482\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss 0.0634769 accuracy 0.4826\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss 0.0477602 accuracy 0.4774\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss 0.0371929 accuracy 0.486\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss 0.0857961 accuracy 0.483\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss 0.0440454 accuracy 0.4918\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss 0.0635607 accuracy 0.4912\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss 0.0617752 accuracy 0.4758\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.45830078125\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERERiTQ5FhERERGJNDkWEREREYn+P4F43cXJY8o1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f98d10386a0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
